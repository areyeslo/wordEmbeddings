{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arturo.reyeslopez\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import word2vec\n",
    "import re \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from itertools import chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (2.1.8)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy) (0.2.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy) (2.21.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy) (7.0.8)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy) (1.16.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy) (0.1.0)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy) (0.2.2)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.31.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (3.4)\n",
      "Requirement already satisfied: six in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from nltk) (3.4.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stem in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: word2vec in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (0.9.4+2.g8204e5c)\n",
      "Requirement already satisfied: numpy in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from word2vec) (1.16.2)\n",
      "Requirement already satisfied: cython in c:\\users\\arturo.reyeslopez\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from word2vec) (0.29.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://dictionary.cambridge.org/grammar/british-grammar/word-formation/prefixes\n",
    "english_prefixes = {\n",
    "\"anti\": \"\",    # e.g. anti-goverment, anti-racist, anti-war\n",
    "\"auto\": \"\",    # e.g. autobiography, automobile\n",
    "\"bio\": \"\",     # e.g. life, living matter\n",
    "#\"de\": \"\",      # e.g. de-classify, decontaminate, demotivate\n",
    "\"dis\": \"\",     # e.g. disagree, displeasure, disqualify\n",
    "\"down\": \"\",    # e.g. downgrade, downhearted\n",
    "\"extra\": \"\",   # e.g. extraordinary, extraterrestrial\n",
    "\"hyper\": \"\",   # e.g. hyperactive, hypertension\n",
    "#\"il\": \"\",     # e.g. illegal\n",
    "#\"im\": \"\",     # e.g. impossible\n",
    "#\"in\": \"\",     # e.g. insecure\n",
    "#\"ir\": \"\",     # e.g. irregular\n",
    "\"inter\": \"\",  # e.g. interactive, international\n",
    "\"mega\": \"\",   # e.g. megabyte, mega-deal, megaton\n",
    "\"mid\": \"\",    # e.g. midday, midnight, mid-October\n",
    "\"mis\": \"\",    # e.g. misaligned, mislead, misspelt\n",
    "\"non\": \"\",    # e.g. non-payment, non-smoking\n",
    "\"over\": \"\",  # e.g. overcook, overcharge, overrate\n",
    "\"out\": \"\",    # e.g. outdo, out-perform, outrun\n",
    "\"post\": \"\",   # e.g. post-election, post-warn\n",
    "\"pre\": \"\",    # e.g. prehistoric, pre-war\n",
    "\"pro\": \"\",    # e.g. pro-communist, pro-democracy\n",
    "#\"re\": \"\",     # e.g. reconsider, redo, rewrite\n",
    "\"semi\": \"\",   # e.g. semicircle, semi-retired\n",
    "\"sub\": \"\",    # e.g. submarine, sub-Saharan\n",
    "\"super\": \"\",   # e.g. super-hero, supermodel\n",
    "\"tele\": \"\",    # e.g. television, telephathic\n",
    "\"trans\": \"\",   # e.g. transatlantic, transfer\n",
    "\"ultra\": \"\",   # e.g. ultra-compact, ultrasound\n",
    "#\"un\": \"\",      # e.g. under-cook, underestimate\n",
    "\"up\": \"\",      # e.g. upgrade, uphill\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_closestwords_tsnescatterplot(model, word, size):\n",
    "    \n",
    "    arr = np.empty((0,size), dtype='f')\n",
    "    word_labels = [word]\n",
    "\n",
    "    close_words = model.similar_by_word(word)\n",
    "\n",
    "    arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get average sentence vector\n",
    "def avg_sentence_vector(words, model, num_features, index2word_set):\n",
    "    #function to average all words vectors in a given paragraph\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    for word in words:\n",
    "        if any(word in s for s in index2word_set):\n",
    "            nwords = nwords+1\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "\n",
    "    if nwords>0:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    \n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_string(x):\n",
    "    return x.lower().replace('.', ' ').replace('-',' ').replace(\"'\", ' ').replace('/',' ').replace('&',' ').replace('(','').replace(')','').replace('?','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def change_string(s):\n",
    "#    return re.sub(r'[^\\w\\s]','',s).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out stopwords\n",
    "# Receives tokenized words\n",
    "def stop_words(word_tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [w for w in word_tokens if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten one level of nesting\n",
    "def flatten(listOfLists):\n",
    "    return list(chain.from_iterable(listOfLists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listOfLists(list):\n",
    "    return list(map(lambda el:[el], lst)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemall(tokens):\n",
    "    ps = PorterStemmer()\n",
    "    return flatten([ [ ps.stem(word) for word in line.split(\" \")] for line in tokens ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recives a list of words\n",
    "#Returns a list of prefixes and steam words\n",
    "def stem_prefix(company, prefixes):\n",
    "    comp=[]\n",
    "    for word in company:\n",
    "        for prefix in sorted(prefixes, key=len, reverse=True):\n",
    "            # Use subn to track the no. of substitution made.\n",
    "            # Allow dash in between prefix and root. \n",
    "            word, nsub = re.subn(\"{}[\\-]?\".format(prefix), \"\", word)\n",
    "            if nsub > 0:\n",
    "                comp.append(prefix)\n",
    "        comp.append(porter.stem(word))\n",
    "    return comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Receives a list of words\n",
    "#Returns a list of prefixes and list of steam words\n",
    "def porter_english_plus(companiesList, prefixes=english_prefixes):\n",
    "    coList=[]\n",
    "    \n",
    "    for company in companiesList:\n",
    "        coList.append(stem_prefix(company, prefixes))\n",
    "    \n",
    "    return coList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search new words to compare against the dataset are included in the vocabulary. \n",
    "#Otherwise, Include them to calculate vectors\n",
    "def searchVocab(words):\n",
    "    vocab=[]\n",
    "    for word in words:\n",
    "        try:\n",
    "            word2vec_model[word]\n",
    "        except:\n",
    "            vocab.append(word)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of words\n",
    "#Clean words:\n",
    "    #1) Removing characters such as .,-,/, etc\n",
    "    #2) Lowering case\n",
    "    #3) Creating tokens\n",
    "    #4) Deleting stop words\n",
    "    #5) Separate prefixes\n",
    "    #5) Stemming tokens\n",
    "def preprocessing(words):\n",
    "    #1,2,3,4\n",
    "    tokens= list(map(stop_words,list(map(nltk.word_tokenize, list(map(lambda x: change_string(x), words))))))\n",
    "    \n",
    "    #5\n",
    "    toks= porter_english_plus(tokens)\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    #6)\n",
    "    return list(map(lambda x: [ps.stem(y) for y in x], toks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read dataset:\n",
    "def readFileToDataFrame(fileName):\n",
    "    return pd.read_csv(fileName, usecols = ['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process dataset sent as dataframe:\n",
    "def preprocess(df):\n",
    "    words=[]\n",
    "    words= df['name'].values.tolist()\n",
    "    \n",
    "    prepWords= preprocessing(words)\n",
    "    \n",
    "    prepWords= [list(filter(None, lst)) for lst in prepWords]\n",
    "    \n",
    "    data_transposed = zip(prepWords)\n",
    "    df = pd.DataFrame(data_transposed, columns=[\"name\"])\n",
    "    \n",
    "    df2 = df['name']\n",
    "    \n",
    "    return prepWords, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(prepWords):\n",
    "    prepWords=flatten(prepWords)\n",
    "    \n",
    "    prepWords= [i for i in prepWords if i] \n",
    "    \n",
    "    prepWords= [[el] for el in prepWords]\n",
    "    \n",
    "    #Training model using preprocessed data\n",
    "    word2vec_model = Word2Vec(prepWords,size=100, window=1, min_count=1, workers=7)\n",
    "\n",
    "    #Summarize vocabulary\n",
    "    words = list(word2vec_model.wv.vocab)\n",
    "    \n",
    "    return word2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arturo.reyeslopez\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "df= readFileToDataFrame('name-submission-sample.csv')\n",
    "\n",
    "col_names =  ['name', 'avg_vector']\n",
    "df2 = pd.DataFrame(col_names)\n",
    "\n",
    "prepWords, df2= preprocess(df)\n",
    "\n",
    "word2vec_model= trainModel(prepWords)\n",
    "\n",
    "df['avg_vector']=df2.apply(lambda row : avg_sentence_vector(row, model=word2vec_model, num_features=100, index2word_set=set(word2vec_model.wv.index2word)).tolist())\n",
    "\n",
    "##Saving name and vector values in file\n",
    "df.to_csv('name-submission-vectors.csv',encoding='utf-8', index=False)\n",
    "\n",
    "words = list(word2vec_model.wv.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arturo.reyeslopez\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n",
      "C:\\Users\\arturo.reyeslopez\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.6677688069184347\n",
      "                      name                                         avg_vector  \\\n",
      "113  FALL TRACTOR SERVICES  [-0.000304568704450503, 0.0021610367111861706,...   \n",
      "\n",
      "     similarity  \n",
      "113    0.667769  \n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#ORIGINAL 4 PAWS ONLY PET SERVICE\n",
    "#get average vector for a new company\n",
    "#company = \"SMOKERY-FLURER\" #1.0\n",
    "#company = \"PAWS 4 PET SERVICE\" #Google:0.95, Word2Vec model:  1.0\n",
    "#company = \"PAWS FOR ONLY PET\" #0.80 0.77193\n",
    "#company = \"PAWS FOR ONLY PETS\" #0.80 0.77193  \n",
    "#company = \"POS FOR PET\" POS is not in the vocabulary\n",
    "#company =\"VICTORIA INSTITUTE BIO CHEMICAL\" #1.0 VICTORIA INSTITUTE BIOCHEMICAL\n",
    "#company =\"VICTORIA INSTITUTE'S BIO-CHEMICAL\"\n",
    "#company = \"AIR TWO RENTAL SOLUTIONS\" #0.745764\n",
    "\n",
    "company= \"FALLEN TRACTOR SERVICES\"#0.701912\n",
    "\n",
    "words=[]\n",
    "words.append(company)\n",
    "\n",
    "prepWords= preprocessing(words)\n",
    "\n",
    "#Before query words in vocabulary and returns the words needed to be included in the vocabulary\n",
    "prepWords=flatten(prepWords)\n",
    "\n",
    "prepWords=[i for i in prepWords if i] \n",
    "\n",
    "vocab= searchVocab(prepWords)\n",
    "#If needed add word to vocabulary\n",
    "if len(vocab):\n",
    "    df= readFileToDataFrame('name-submission-sample.csv')\n",
    "    prepWords, df2= preprocess(df)\n",
    "    prepWords.append(vocab)\n",
    "    word2vec_model= trainModel(prepWords)\n",
    "    \n",
    "    #Calculating average vector for each company name:\n",
    "    df['avg_vector']=df2.apply(lambda row : avg_sentence_vector(row, model=word2vec_model, num_features=100, index2word_set=set(word2vec_model.wv.index2word)).tolist())\n",
    "\n",
    "    #Saving name and vector values in file\n",
    "    df.to_csv('name-submission-vectors-new-copy.csv',encoding='utf-8', index=False)\n",
    "\n",
    "else:\n",
    "    df = pd.read_csv('name-submission-vectors.csv', usecols = ['name','avg_vector'])\n",
    "    df['avg_vector']=df['avg_vector'].apply(lambda s: [float(x.strip(' []')) for x in s.split(',')])\n",
    "\n",
    "\n",
    "company_avg_vector = avg_sentence_vector(prepWords, model=word2vec_model, num_features=100, index2word_set=set(word2vec_model.wv.index2word)).tolist()\n",
    "\n",
    "df['similarity']=df['avg_vector'].apply(lambda row : 1 - spatial.distance.cosine(row, company_avg_vector))\n",
    "\n",
    "s= df['similarity'].max()\n",
    "print(\"Similarity:\",s)\n",
    "\n",
    "df3= df.loc[df['similarity']== s]\n",
    "print(df3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word2vec_model['victoria'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(word2vec_model.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (word2vec_model.similarity('bio', 'biochem'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (word2vec_model.most_similar(positive=['biochem'], negative=[], topn=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_closestwords_tsnescatterplot(word2vec_model, 'biochem', 100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
